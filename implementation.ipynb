{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Overview"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ba59f86002305b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"transport_shipment_data.csv\")\n",
    "df = df.drop('Product Id', axis=1)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed9d191d21a0e34f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.dtypes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a50b836546594a5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6034d627d1af85b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9705a8693ae6e8e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling Missing Values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1edd3fc7d0a339fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "missing_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18818f05f1fe7efc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "missing_data_proportion = df.isnull().mean() * 100\n",
    "missing_data_proportion = missing_data_proportion[missing_data_proportion > 0]\n",
    "missing_data_proportion"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2425e1495ccea883"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Expiry Period (5.00%): Since the percentage of missing data is low, imputation would also be a simple solution, using the mean or median.\n",
    "\n",
    "Perishable Index (2.35%): With such a low proportion of missing data, imputation would also be a simple solution, using the mean or median.\n",
    "\n",
    "F992 (37.30%):With a high proportion of missing values, you have two main options:\n",
    "\t•\tImputation: You could use more advanced imputation techniques such as KNN imputation or predictive imputation, though basic methods (like the mean) could also work if it’s important to keep this feature.\n",
    "\t•\tDropping the Column: If this feature isn’t essential or strongly correlated with your target variable, dropping it might be a viable option to avoid introducing noise."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25bdab2698d3c914"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the correlation between F992 and the transport mode variables (Air, Road, Rail, Sea)\n",
    "correlation_with_target = df[['F992', 'Air', 'Road', 'Rail', 'Sea']].corr()\n",
    "# Display the correlation values between F992 and each of the transport mode columns\n",
    "correlation_with_target"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9082eb5529321325"
  },
  {
   "cell_type": "markdown",
   "source": [
    "F992 vs. Air: -0.037 (very weak negative correlation)\n",
    "F992 vs. Road: -0.293 (moderate negative correlation)\n",
    "F992 vs. Rail: 0.359 (moderate positive correlation)\n",
    "F992 vs. Sea: 0.124 (weak positive correlation)\n",
    "\n",
    "F992 has a moderate positive correlation with Rail (0.359) and a moderate negative correlation with Road (-0.293). These suggest that F992 has some influence when predicting Rail and Road transport modes.\n",
    "\t•\tThe correlations with Air and Sea are quite weak, indicating F992 may not be as important for predicting those modes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9810ce990afcea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['F992'].fillna(df['F992'].mean(), inplace=True)\n",
    "df['Expiry Period'].fillna(df['Expiry Period'].mean(), inplace=True)\n",
    "df['Perishable Index'].fillna(df['Perishable Index'].mean(), inplace=True)\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "missing_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f05b70297d420f07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Target Variable Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d761fee68e111d52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking the distribution of the target variables (Air, Road, Rail, Sea) using .value_counts()\n",
    "target_distribution = df[['Air', 'Road', 'Rail', 'Sea']].sum()\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "target_distribution.plot(kind='bar', color='blue', alpha=0.5)\n",
    "plt.title('Distribution of Transport Mode Classes')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Transport Mode')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "target_distribution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f655943e755d2e06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is still some imbalance (especially with Road having significantly more instances than the other classes), it is not as extreme. Some models (like RandomForest or XGBoost) are more robust to class imbalances and may not require resampling.\n",
    "\n",
    "Support Vector Machines (SVM) with One-vs-Rest can work well with high-dimensional data and perform well in binary classification tasks but computationally expensive for large datasets.\n",
    "Logistic Regression with One-vs-Rest (OvR) can be a strong baseline model, especially when using One-vs-Rest for multiclass classification but may underperform if the relationships between features are non-linear or complex. SMOTE or oversampling techniques may be needed if using models that do not naturally handle imbalance well (Logistic Regression, SVM)\n",
    "\n",
    "LGBMClassifier and XGBoostClassifier are being chosen as the models to be trained as they handle class imbalance through their objective functions, perform well on structured data and are efficient for large datasets.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d0c8057a66a46b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Univariate Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "492134ac1b6f7eca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "print(\"Categorical Features:\")\n",
    "print(categorical_features)\n",
    "\n",
    "print(\"\\nDistinct values for each categorical feature:\")\n",
    "for feature in categorical_features:\n",
    "    distinct_values = df[feature].nunique()\n",
    "    unique_values = df[feature].unique()\n",
    "    print(f\"{feature}: {distinct_values} unique values\")\n",
    "    print(f\"Values: {unique_values}\")\n",
    "\n",
    "print(\"\\nNumerical Features:\")\n",
    "print(numerical_features)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dc4a09dc6c8c46c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Univariate Analysis - Numerical Features\n",
    "numerical_columns = ['Net Weight', 'Value', 'Packaging Cost', 'Expiry Period', 'Length', \n",
    "                     'Height', 'Width', 'Volume', 'Perishable Index', 'Flammability Index', 'F145', 'F7987', 'F992']\n",
    "\n",
    "# Plotting histograms and box plots for numerical features\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    plt.subplot(5, 4, i+1)\n",
    "    sns.histplot(df[col], kde=True, color='blue')\n",
    "    plt.title(f'Histogram and KDE for {col}')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "143cc1d519c7e660"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Categorical Features (Size)\n",
    "categorical_columns = ['Size']\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Size', data=df, color='blue', alpha=0.5)\n",
    "plt.title('Bar Plot for Size')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67bb8b796ccac03f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outlier Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b6ff56a392897f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    plt.subplot(5, 4, i+1)\n",
    "    sns.boxplot(x=df[col], color='skyblue')\n",
    "    plt.title(f'Box Plot for {col}')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7675268198d2c7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, columns):\n",
    "    outliers = {}\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers[col] = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "    return outliers\n",
    "\n",
    "outliers = detect_outliers_iqr(df, numerical_columns)\n",
    "outliers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae9597d9bdf231a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "No outliers detected for most of the numerical columns like Net Weight, Value, Packaging Cost, Expiry Period, etc.\n",
    "Column F7987: 207 outliers detected.\n",
    "Column F992: 361 outliers detected."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61026e02715b05ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate skewness for numerical columns\n",
    "skewness = df[numerical_columns].skew()\n",
    "print(skewness)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae162ab2230a6423"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Symmetrical: Features with skewness close to 0 (between -0.5 and 0.5) are considered roughly symmetric, meaning they are not significantly skewed.\n",
    "\t•\tNet Weight: -0.014 (very symmetric)\n",
    "\t•\tValue: 0.283 (slightly positive skew)\n",
    "\t•\tPackaging Cost: 0.179 (slightly positive skew)\n",
    "\t•\tExpiry Period: -0.279 (slightly negative skew)\n",
    "\t•\tLength: 0.036 (very symmetric)\n",
    "\t•\tHeight: 0.263 (slightly positive skew)\n",
    "\t•\tWidth: -0.439 (slightly negative skew)\n",
    "\t•\tVolume: -0.163 (slightly negative skew)\n",
    "\t•\tPerishable Index: -0.0008 (very symmetric)\n",
    "\t•\tFlammability Index: 0.021 (very symmetric)\n",
    "\t•\tF145: 0.0069 (very symmetric)\n",
    "\t•\tF992: -0.211 (slightly negative skew)\n",
    "\n",
    "Moderate Skew: A few features exhibit moderate skewness (between 0.5 and 1 or -1 and -0.5):\n",
    "\t•\tF7987: 0.305 (slightly positive skew)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38bdaa267d77102"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multivariate Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "963bf697857163bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multicollinearity Check"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14e6ff3d81113c9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numerical_df = df[numerical_columns]\n",
    "pearson_corr_matrix = numerical_df.corr(method='pearson')\n",
    "spearman_corr_matrix = numerical_df.corr(method='spearman')\n",
    "\n",
    "print(\"Pearson Correlation Matrix:\")\n",
    "print(pearson_corr_matrix)\n",
    "\n",
    "print(\"\\nSpearman Correlation Matrix:\")\n",
    "print(spearman_corr_matrix)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.heatmap(pearson_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', ax=axes[0])\n",
    "axes[0].set_title('Pearson Correlation Matrix')\n",
    "sns.heatmap(spearman_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', ax=axes[1])\n",
    "axes[1].set_title('Spearman Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51e3ba56586f1d47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlation analysis with the target variable\n",
    "Checking the correlation between each feature and the individual target classes (Air, Road, Rail, Sea) using Pearson or Spearman correlations for numerical targets, and techniques like ANOVA or Chi-square for categorical targets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce7b5862ffed5e21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Correlation of numerical features with all 4 target variables (Air, Road, Rail, Sea)\n",
    "correlation_with_targets = {}\n",
    "\n",
    "for target in ['Air', 'Road', 'Rail', 'Sea']:\n",
    "    correlation_with_targets[target] = df[numerical_columns].corrwith(df[target])\n",
    "\n",
    "correlation_with_targets_df = pd.DataFrame(correlation_with_targets)\n",
    "correlation_with_targets_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94d8de55548506a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Insights:\n",
    "\t•\tNet Weight shows a moderate positive correlation with Rail (0.374) and a negative correlation with Road (-0.355).\n",
    "\t•\tValue is positively correlated with Rail (0.475), but negatively correlated with Air (-0.343).\n",
    "\t•\tHeight has a strong positive correlation with Air (0.418) and a negative correlation with Rail (-0.220).\n",
    "\t•\tWidth is positively correlated with Sea (0.276), while negatively correlated with Road (-0.306).\n",
    "\t•\tPackaging Cost shows a positive correlation with Road (0.281) but is negatively correlated with Rail and Sea.\n",
    "\n",
    "Conclusion:\n",
    "\t•\tFeatures like Net Weight, Value, Height, and Width appear to have the most influence on the different transport modes.\n",
    "\t•\tHeight and Net Weight are especially important for Air and Rail."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b27f9bb1c212d159"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categorical_columns = ['Size']\n",
    "chi_square_results = {}\n",
    "\n",
    "for target in ['Air', 'Road', 'Rail', 'Sea']:\n",
    "    for col in categorical_columns:\n",
    "        contingency_table = pd.crosstab(df[col], df[target])\n",
    "        chi2, p, dof, ex = chi2_contingency(contingency_table)\n",
    "        chi_square_results[f'{col} vs {target}'] = p\n",
    "\n",
    "chi_square_df = pd.DataFrame(chi_square_results.items(), columns=['Feature vs Target', 'p-value'])\n",
    "chi_square_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9ecdc9d33245e11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "p-value > 0.05: For all target variables, the p-values are greater than 0.05, meaning that the feature Size does not show a statistically significant association with any of the target variables. This suggests that Size may not be a strong predictor of the transport mode (Air, Road, Rail, Sea)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d3c63fcccb8fd9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Encoding\n",
    "Categorical Features are encoded.Target Variables (Air, Road, Rail, Sea) are already represented as binary (0/1), so no encoding is required for the target."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "552c87ac6bd85a99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['Size'] = label_encoder.fit_transform(df['Size'])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3285f4d06f0cb173"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Development"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad6e7762c078cb1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train models using StratifiedKFold\n",
    "def train_models(models_dict, X, y_labels, skf, evaluation_reports, cv_f1_scores):\n",
    "    for model_name, model in models_dict.items():\n",
    "        for label in y_labels:\n",
    "\n",
    "            fold_f1_scores = []  # Store F1 scores for each fold\n",
    "            fold_reports = []  # Store classification reports for each fold\n",
    "\n",
    "            # Stratified K-Fold cross-validation\n",
    "            for train_index, test_index in skf.split(X, df[label]):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = df[label].iloc[train_index], df[label].iloc[test_index]\n",
    "\n",
    "                model.fit(X_train, y_train)  # Train the model\n",
    "\n",
    "                # Predict the target on the test set\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                # Calculate weighted F1 score and store for each fold\n",
    "                fold_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                fold_f1_scores.append(fold_f1)\n",
    "\n",
    "                # Store the classification report\n",
    "                try:\n",
    "                    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "                    fold_reports.append(report)\n",
    "                except ValueError:\n",
    "                    fold_reports.append({})  # Add an empty dict if the report fails\n",
    "\n",
    "            # Store F1 scores and reports for each label\n",
    "            cv_f1_scores[model_name][label] = fold_f1_scores\n",
    "            evaluation_reports[model_name][label] = fold_reports\n",
    "\n",
    "# Calculate the mean F1 scores across folds\n",
    "def calculate_mean_f1_scores(cv_f1_scores, y_labels):\n",
    "    mean_f1_scores_df = pd.DataFrame({\n",
    "        model_name: {label: np.mean(cv_f1_scores[model_name][label]) for label in y_labels}\n",
    "        for model_name in models_dict.keys()\n",
    "    })\n",
    "    return mean_f1_scores_df\n",
    "\n",
    "def average_metrics_across_folds(reports):\n",
    "    \"\"\"Compute the average metrics across folds.\"\"\"\n",
    "    avg_report = {\n",
    "        '0': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "        '1': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "        'accuracy': []\n",
    "    }\n",
    "\n",
    "    # Accumulate metrics for each fold\n",
    "    for report in reports:\n",
    "        for cls in ['0', '1']:\n",
    "            avg_report[cls]['precision'].append(report[cls]['precision'])\n",
    "            avg_report[cls]['recall'].append(report[cls]['recall'])\n",
    "            avg_report[cls]['f1-score'].append(report[cls]['f1-score'])\n",
    "            avg_report[cls]['support'].append(report[cls]['support'])\n",
    "        avg_report['accuracy'].append(report['accuracy'])\n",
    "    \n",
    "    # Compute average values, using np.nanmean to handle potential NaN values\n",
    "    for cls in ['0', '1']:\n",
    "        avg_report[cls]['precision'] = np.nanmean(avg_report[cls]['precision'])\n",
    "        avg_report[cls]['recall'] = np.nanmean(avg_report[cls]['recall'])\n",
    "        avg_report[cls]['f1-score'] = np.nanmean(avg_report[cls]['f1-score'])\n",
    "        avg_report[cls]['support'] = np.nanmean(avg_report[cls]['support'])\n",
    "    avg_report['accuracy'] = np.nanmean(avg_report['accuracy'])\n",
    "    \n",
    "    return avg_report\n",
    "\n",
    "def generate_classification_report_tables(evaluation_reports):\n",
    "    \"\"\"Generate and display tables with averaged metrics across folds.\"\"\"\n",
    "    for model_name, labels in evaluation_reports.items():\n",
    "        for label_name, reports in labels.items():\n",
    "            print(f\"\\n=== {model_name} - {label_name} Label ===\")\n",
    "            \n",
    "            if reports:\n",
    "                # Average out the metrics across the folds\n",
    "                avg_report = average_metrics_across_folds(reports)\n",
    "                \n",
    "                # Create the classification report table using the averaged report\n",
    "                table = create_classification_report_table(avg_report, model_name, label_name)\n",
    "                \n",
    "                if not table.empty:\n",
    "                    print(table.to_string(index=False))\n",
    "                else:\n",
    "                    print(f\"Classification report for {model_name} - {label_name} could not be displayed.\")\n",
    "            else:\n",
    "                print(f\"No reports found for {model_name} - {label_name}\")\n",
    "\n",
    "def create_classification_report_table(report_dict, model_name, label_name):\n",
    "    \"\"\"Create a DataFrame from the averaged classification report.\"\"\"\n",
    "    # Extract class 0 and class 1 metrics\n",
    "    class_0 = report_dict['0']\n",
    "    class_1 = report_dict['1']\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Metric': ['Precision', 'Recall', 'F1-score', 'Support'],\n",
    "        f'{model_name} (Class 0)': [class_0['precision'], class_0['recall'], class_0['f1-score'], class_0['support']],\n",
    "        f'{model_name} (Class 1)': [class_1['precision'], class_1['recall'], class_1['f1-score'], class_1['support']],\n",
    "    })\n",
    "    \n",
    "    # Adding accuracy as a separate row\n",
    "    df = pd.concat([df, pd.DataFrame({\n",
    "        'Metric': ['Accuracy'],\n",
    "        f'{model_name} (Class 0)': [report_dict['accuracy']],\n",
    "        f'{model_name} (Class 1)': [np.nan]  # Accuracy is global, not per class\n",
    "    })], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_comparison(metric, evaluation_dfs):\n",
    "    \"\"\"\n",
    "    Plots the comparison of the specified metric across models for each label.\n",
    "    The metric can be 'accuracy', 'precision', 'recall', or 'f1-score'.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for model_name, labels in evaluation_dfs.items():\n",
    "        metric_values = []  # To store the metric values for each label\n",
    "        \n",
    "        for label in y_labels:\n",
    "            # Get the first report (as the example assumes we are working with the first fold's report)\n",
    "            reports_for_label = labels[label]\n",
    "            \n",
    "            if reports_for_label:\n",
    "                report = reports_for_label[0]  # Use the first fold's report\n",
    "                \n",
    "                if metric == 'accuracy':\n",
    "                    # Directly get the accuracy metric from the report\n",
    "                    metric_value = report['accuracy']\n",
    "                else:\n",
    "                    # For other metrics like precision, recall, f1-score, get the class 0 average\n",
    "                    metric_value = report['0'][metric]\n",
    "                \n",
    "                metric_values.append(metric_value)\n",
    "            else:\n",
    "                metric_values.append(None)  # If there's no report for this label\n",
    "        \n",
    "        plt.plot(y_labels, metric_values, marker='o', label=model_name)\n",
    "    \n",
    "    plt.title(f'Comparison of {metric} across models')\n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff70b01cb876280d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the models to be trained\n",
    "models_dict = {\n",
    "    'LightGBM': lgb.LGBMClassifier(class_weight='balanced', verbose=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', verbosity=0),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0)\n",
    "}\n",
    "\n",
    "# Initialize evaluation dictionaries to store cross-validation reports and F1 scores\n",
    "evaluation_reports = {model_name: {label: [] for label in ['Air', 'Road', 'Rail', 'Sea']} for model_name in models_dict.keys()}\n",
    "cv_f1_scores = {model_name: {label: [] for label in ['Air', 'Road', 'Rail', 'Sea']} for model_name in models_dict.keys()}\n",
    "\n",
    "# Features (X) and target labels (y)\n",
    "X = df.drop(columns=['Air', 'Road', 'Rail', 'Sea'])  # Define features\n",
    "y_labels = ['Air', 'Road', 'Rail', 'Sea']  # Define target labels\n",
    "\n",
    "# Create StratifiedKFold object for 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_models(models_dict, X, y_labels, skf, evaluation_reports, cv_f1_scores)\n",
    "\n",
    "# Calculate mean F1 scores and output results\n",
    "mean_f1_scores_df = calculate_mean_f1_scores(cv_f1_scores, y_labels)\n",
    "print(\"\\nMean F1 Scores Across Models and Labels:\")\n",
    "print(mean_f1_scores_df)\n",
    "\n",
    "# Display classification reports as tables\n",
    "generate_classification_report_tables(evaluation_reports)\n",
    "\n",
    "# Example plot: Compare accuracy across models\n",
    "plot_comparison('accuracy', evaluation_reports)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f07645a1032a6c69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccac97d44962feb5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_comparison('f1-score', evaluation_reports) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2432c91d6244a264"
  },
  {
   "cell_type": "markdown",
   "source": [
    "CatBoost consistently outperforms LightGBM and XGBoost across all labels, particularly for the minority classes (Class 1). This suggests that CatBoost might be better suited for handling class imbalance or more complex data distributions.\n",
    "\n",
    "he Sea label poses the greatest challenge for all models, as indicated by the relatively lower F1 scores, especially for Class 1.\n",
    "\t•\tClass 0 (likely the majority class) is consistently easier to classify across all labels and models, achieving high precision, recall, and F1 scores.\n",
    "\n",
    "\n",
    "Accuracy:\n",
    "- **Best Model**: CatBoost achieves the highest accuracy across the majority of labels, especially for “Air” (0.900), “Road” (0.905), and “Sea” (0.901).\n",
    "- LightGBM and XGBoost perform similarly but slightly lag behind CatBoost in overall accuracy.\n",
    "- CatBoost consistently outperforms LightGBM and XGBoost across all transport modes, especially for Sea transport, which appears to be the most challenging class.he differences between the models are small but meaningful, and CatBoost’s ability to handle categorical data and class imbalance without manual intervention gives it a slight advantage.\n",
    "\n",
    "Cross-Validation:\n",
    "- **Best Generalization**: CatBoost also demonstrates superior generalization with the highest cross-validation scores across all labels except “Sea.” This indicates that CatBoost is more consistent in making predictions across different data splits.\n",
    "- XGBoost performs well for “Air” and “Rail,” while LightGBM is quite competitive but slightly less consistent for “Sea.”\n",
    "\n",
    "CatBoost is the most reliable model for handling imbalanced datasets and can be favored for deployment.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c75b1ab9be632a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64375c97bb5c8455"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_feature_importance(models_dict, feature_names):\n",
    "    feature_importance_df = pd.DataFrame(index=feature_names)\n",
    "    for model_name, model in models_dict.items():\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = model.feature_importances_\n",
    "        else:\n",
    "            importance = model.get_feature_importance()\n",
    "\n",
    "        feature_importance_df[model_name] = importance\n",
    "        indices = np.argsort(importance)[::-1]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(f\"{model_name} Feature Importance\")\n",
    "        plt.bar(range(len(feature_names)), importance[indices], align='center',color='blue', alpha=0.5)\n",
    "        plt.xticks(range(len(feature_names)), np.array(feature_names)[indices], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "feature_importance_df = plot_feature_importance(models_dict, feature_names)\n",
    "print(\"Feature Importance Values:\")\n",
    "print(feature_importance_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df6435ee8d4aeb07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Insights:\n",
    "\t•\tLightGBM: Feature importance in LightGBM is on a larger scale, which is normal for this model as the feature importance values are based on the number of times a feature is used in splits across all trees. For example:\n",
    "\t•\tNet Weight: High importance with a score of 307.\n",
    "\t•\tWidth, Flammability Index, and Value are also quite important.\n",
    "\t•\tXGBoost: The feature importance in XGBoost is expressed on a smaller scale (relative to LightGBM), as it reflects how useful a feature is in reducing the loss. Key features include:\n",
    "\t•\tValue: The most important feature in XGBoost, with a score of 0.1136.\n",
    "\t•\tWidth and Packaging Cost also have significant importance.\n",
    "\t•\tCatBoost: CatBoost calculates feature importance differently and focuses on how each feature impacts the model’s predictions. In CatBoost:\n",
    "\t•\tWidth: The most important feature, with a score of 13.28.\n",
    "\t•\tValue, Packaging Cost, and Height are also crucial.\n",
    "\tPhysical attributes (e.g., Net Weight, Length, Width, Height) and monetary value-related features (e.g., Value, Packaging Cost) are key drivers in predicting transport modes.\n",
    "\t•\tCatBoost places higher importance on most features, particularly on dimensions and packaging costs, suggesting it captures more complex interactions between features.\n",
    "\t•\tLightGBM has more variance in importance values, focusing strongly on a few key features like Width and Packaging Cost, while XGBoost distributes importance more evenly across features.\n",
    "\n",
    "Conclusion:\n",
    "\t•\tCommon Important Features: Across all models, Value, Width, Packaging Cost, and Height consistently appear as important features, although their relative importance varies depending on the model.\n",
    "\t\n",
    "The feature importance results suggest that most of the features are already contributing well to the model, especially in CatBoost. Removing lower-importance features like Storage or Size might marginally reduce model complexity but likely won’t bring significant performance improvements."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e5d368fed6ebab9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Tuning for CatBoost"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e58611a92acae4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Objective function for optimization\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters for optimization\n",
    "    param = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1, 10),\n",
    "        'iterations': trial.suggest_int('iterations', 500, 1500),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 128),\n",
    "        'random_strength': trial.suggest_loguniform('random_strength', 0.1, 10),\n",
    "        'task_type': 'CPU',  # Or GPU if supported\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    # Define the model\n",
    "    catboost_model = CatBoostClassifier(**param)\n",
    "\n",
    "    # Perform cross-validation and minimize negative F1 score\n",
    "    f1 = cross_val_score(catboost_model, X, y, cv=3, scoring='f1_weighted').mean()\n",
    "    return f1\n",
    "\n",
    "# Define an Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Start optimization\n",
    "study.optimize(objective, n_trials=50)  # Run 50 trials\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best parameters:\", study.best_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f03d0c4a88c914"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
